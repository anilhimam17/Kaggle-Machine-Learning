{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698c54c7",
   "metadata": {
    "papermill": {
     "duration": 0.012506,
     "end_time": "2024-05-18T19:52:02.633848",
     "exception": false,
     "start_time": "2024-05-18T19:52:02.621342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Necessary Imports**\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebbffef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:52:02.661113Z",
     "iopub.status.busy": "2024-05-18T19:52:02.660248Z",
     "iopub.status.idle": "2024-05-18T19:52:18.644986Z",
     "shell.execute_reply": "2024-05-18T19:52:18.643688Z"
    },
    "papermill": {
     "duration": 16.00181,
     "end_time": "2024-05-18T19:52:18.648086",
     "exception": false,
     "start_time": "2024-05-18T19:52:02.646276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 19:52:04.838109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-18 19:52:04.838244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-18 19:52:04.997538: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3001788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:52:18.675707Z",
     "iopub.status.busy": "2024-05-18T19:52:18.674632Z",
     "iopub.status.idle": "2024-05-18T19:52:19.177010Z",
     "shell.execute_reply": "2024-05-18T19:52:19.175935Z"
    },
    "papermill": {
     "duration": 0.518929,
     "end_time": "2024-05-18T19:52:19.179667",
     "exception": false,
     "start_time": "2024-05-18T19:52:18.660738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train:  (11610, 8)\n",
      "X Valid:  (3870, 8)\n",
      "X Test:  (5160, 8)\n"
     ]
    }
   ],
   "source": [
    "housing_data = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_data.data, housing_data.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train)\n",
    "\n",
    "std_sc = StandardScaler()\n",
    "X_train = std_sc.fit_transform(X_train)\n",
    "X_valid = std_sc.fit_transform(X_valid)\n",
    "X_test = std_sc.fit_transform(X_test)\n",
    "\n",
    "print(\"X Train: \", X_train.shape)\n",
    "print(\"X Valid: \", X_valid.shape)\n",
    "print(\"X Test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79feae",
   "metadata": {
    "papermill": {
     "duration": 0.012255,
     "end_time": "2024-05-18T19:52:19.204892",
     "exception": false,
     "start_time": "2024-05-18T19:52:19.192637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Building a Residual Network (ResNet)**\n",
    "--\n",
    "\n",
    "<center>\n",
    "<img src=\"https://d2l.ai/_images/resnet-block.svg\" alt=\"Image of ResNet\">\n",
    "</center>\n",
    "\n",
    "The above image presents a common architecture utilising **`ResNets`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8b5dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:52:19.232060Z",
     "iopub.status.busy": "2024-05-18T19:52:19.230998Z",
     "iopub.status.idle": "2024-05-18T19:52:19.240257Z",
     "shell.execute_reply": "2024-05-18T19:52:19.239387Z"
    },
    "papermill": {
     "duration": 0.025333,
     "end_time": "2024-05-18T19:52:19.242591",
     "exception": false,
     "start_time": "2024-05-18T19:52:19.217258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the Custom Residual Layer\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        \n",
    "        # Inheriting all the attributes from the parent class\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Defining the Residual Layer\n",
    "        self.hidden = [\n",
    "            tf.keras.layers.Dense(n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\") for layer in range(n_layers)\n",
    "        ]\n",
    "        \n",
    "        '''\n",
    "        The above list comprehension defines all the Dense layers in between the skip connection of the Residual block\n",
    "        '''\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        \n",
    "        # Looping to complete forward propagation of the hidden layers in the Residual Block\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "            \n",
    "        # Returns a concatenation of the results from forward prop through the dense layers with the skip connection\n",
    "        return inputs + Z\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"hidden\": self.hidden}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ea832",
   "metadata": {
    "papermill": {
     "duration": 0.013418,
     "end_time": "2024-05-18T19:52:19.268549",
     "exception": false,
     "start_time": "2024-05-18T19:52:19.255131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Important**\n",
    "- When Keras detects the hidden attribute it automatically tallies the parameters of all the hidden layers.\n",
    "- Keras then adds all the tallied parameters to the trackable parameters of custom layer for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac42d16a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:52:19.296368Z",
     "iopub.status.busy": "2024-05-18T19:52:19.295683Z",
     "iopub.status.idle": "2024-05-18T19:52:19.304596Z",
     "shell.execute_reply": "2024-05-18T19:52:19.303794Z"
    },
    "papermill": {
     "duration": 0.02559,
     "end_time": "2024-05-18T19:52:19.307004",
     "exception": false,
     "start_time": "2024-05-18T19:52:19.281414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        \n",
    "        # Inheriting all the attributes from the parent class\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # First Layer of the Residual Regressor Model\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        \n",
    "        # Residual Layers\n",
    "        self.resblock1 = ResidualBlock(2, 30)\n",
    "        self.resblock2 = ResidualBlock(2, 30)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Propagation through the Input Layer\n",
    "        Z = self.hidden1(inputs)\n",
    "        \n",
    "        # Propagation through the first Residual Blocks 4 times \n",
    "        for layer in range(4):\n",
    "            Z = self.resblock1(Z)\n",
    "            \n",
    "        # Propagation through the second Residual Block\n",
    "        Z = self.resblock2(Z)\n",
    "        \n",
    "        # Final Output\n",
    "        return self.out(Z)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"output_dim\": self.out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d60694c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:52:19.334343Z",
     "iopub.status.busy": "2024-05-18T19:52:19.333663Z",
     "iopub.status.idle": "2024-05-18T19:52:44.058529Z",
     "shell.execute_reply": "2024-05-18T19:52:44.057382Z"
    },
    "papermill": {
     "duration": 24.741382,
     "end_time": "2024-05-18T19:52:44.060998",
     "exception": false,
     "start_time": "2024-05-18T19:52:19.319616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"residual_regressor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"residual_regressor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_1                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block (\u001b[38;5;33mResidualBlock\u001b[0m)  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_1                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2.2475 - val_loss: 0.7206\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7112 - val_loss: 0.6658\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5615 - val_loss: 0.6394\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5326 - val_loss: 0.7940\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7577 - val_loss: 0.8395\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4657 - val_loss: 1.0456\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4426 - val_loss: 1.0787\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7416 - val_loss: 1.0922\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4110 - val_loss: 1.3155\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4072 - val_loss: 1.2885\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4223 - val_loss: 1.5047\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3553 - val_loss: 2.0166\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3372 - val_loss: 2.6757\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3482 - val_loss: 2.3055\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3439 - val_loss: 2.3953\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3311 - val_loss: 3.4126\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4533 - val_loss: 3.3567\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4111 - val_loss: 2.5311\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3759 - val_loss: 3.5786\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4026 - val_loss: 3.9494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7b745c81dba0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the Model\n",
    "residual_model = ResidualRegressor(1)\n",
    "residual_model.summary()\n",
    "\n",
    "# Compiling the model\n",
    "residual_model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "residual_model.fit(\n",
    "    X_train, y_train, validation_data=(X_valid, y_valid), epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb21a127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:52:44.153871Z",
     "iopub.status.busy": "2024-05-18T19:52:44.153486Z",
     "iopub.status.idle": "2024-05-18T19:52:44.206103Z",
     "shell.execute_reply": "2024-05-18T19:52:44.205044Z"
    },
    "papermill": {
     "duration": 0.09991,
     "end_time": "2024-05-18T19:52:44.208866",
     "exception": false,
     "start_time": "2024-05-18T19:52:44.108956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "residual_model.save(\"resnet_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722ad63",
   "metadata": {
    "papermill": {
     "duration": 0.043286,
     "end_time": "2024-05-18T19:52:44.295391",
     "exception": false,
     "start_time": "2024-05-18T19:52:44.252105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Losses and Metrics based on model internals**\n",
    "--\n",
    "\n",
    "- When training more complex models we will need to account for losses due to specific behaviour encountered by specific layers of the model.\n",
    "- This is done by monitoring the internal losses of the model in specific parts of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df99f308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:52:44.382489Z",
     "iopub.status.busy": "2024-05-18T19:52:44.382085Z",
     "iopub.status.idle": "2024-05-18T19:52:44.393608Z",
     "shell.execute_reply": "2024-05-18T19:52:44.392500Z"
    },
    "papermill": {
     "duration": 0.057835,
     "end_time": "2024-05-18T19:52:44.396180",
     "exception": false,
     "start_time": "2024-05-18T19:52:44.338345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReconstructionRegressor(tf.keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Defining the hidden layers\n",
    "        self.hidden = [\n",
    "            tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\") for layer in range(5)\n",
    "        ]\n",
    "        \n",
    "        # Defining the output layer\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "        # Reconstruction Mean\n",
    "        self.reconstruction_mean = tf.keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        \n",
    "        # Carrying Forward Propagation\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "            \n",
    "        # Passing the final o/p of forward prop through the reconstruction layer to calculate the reconstruction loss\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        \n",
    "        # Adding the calculated loss to the loss of the model with a 5% weight\n",
    "        self.add_loss(0.05 * reconstruction_loss)\n",
    "        \n",
    "        # Updating the extra metric\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(reconstruction_loss)\n",
    "            \n",
    "        # Output\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b3be29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:52:44.482855Z",
     "iopub.status.busy": "2024-05-18T19:52:44.482427Z",
     "iopub.status.idle": "2024-05-18T19:53:09.349969Z",
     "shell.execute_reply": "2024-05-18T19:53:09.349088Z"
    },
    "papermill": {
     "duration": 24.913377,
     "end_time": "2024-05-18T19:53:09.352227",
     "exception": false,
     "start_time": "2024-05-18T19:52:44.438850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2.1144 - reconstruction_error: 1.2771 - val_loss: 0.6131 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5366 - reconstruction_error: 0.5980 - val_loss: 0.5336 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4458 - reconstruction_error: 0.4101 - val_loss: 0.6151 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4080 - reconstruction_error: 0.3361 - val_loss: 0.6715 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3953 - reconstruction_error: 0.2379 - val_loss: 0.7399 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4070 - reconstruction_error: 0.2932 - val_loss: 0.7916 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3784 - reconstruction_error: 0.2331 - val_loss: 0.8167 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3480 - reconstruction_error: 0.2447 - val_loss: 0.9771 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3328 - reconstruction_error: 0.1794 - val_loss: 1.0591 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3420 - reconstruction_error: 0.1715 - val_loss: 1.2107 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3272 - reconstruction_error: 0.1502 - val_loss: 1.3284 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3280 - reconstruction_error: 0.1710 - val_loss: 1.3897 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3187 - reconstruction_error: 0.1550 - val_loss: 1.4649 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3266 - reconstruction_error: 0.1528 - val_loss: 1.5725 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3487 - reconstruction_error: 0.1925 - val_loss: 1.8124 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3034 - reconstruction_error: 0.1683 - val_loss: 1.6124 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2908 - reconstruction_error: 0.1383 - val_loss: 1.6920 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3098 - reconstruction_error: 0.2985 - val_loss: 1.9620 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2967 - reconstruction_error: 0.1437 - val_loss: 2.0832 - val_reconstruction_error: 0.0000e+00\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2861 - reconstruction_error: 0.1254 - val_loss: 2.0306 - val_reconstruction_error: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7b745c32b4f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the Reconstruction Model\n",
    "rec_model = ReconstructionRegressor(1)\n",
    "\n",
    "# Compiling the model\n",
    "rec_model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=\"nadam\"\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "rec_model.fit(\n",
    "    X_train, y_train, validation_data=(X_valid, y_valid), epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee04cf",
   "metadata": {
    "papermill": {
     "duration": 0.072019,
     "end_time": "2024-05-18T19:53:09.496344",
     "exception": false,
     "start_time": "2024-05-18T19:53:09.424325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Computing Gradient using the TensorFlow AutoDiff**\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23690b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:09.643099Z",
     "iopub.status.busy": "2024-05-18T19:53:09.641821Z",
     "iopub.status.idle": "2024-05-18T19:53:09.649566Z",
     "shell.execute_reply": "2024-05-18T19:53:09.648499Z"
    },
    "papermill": {
     "duration": 0.083628,
     "end_time": "2024-05-18T19:53:09.651848",
     "exception": false,
     "start_time": "2024-05-18T19:53:09.568220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6300\n"
     ]
    }
   ],
   "source": [
    "# Working on Differentiating a Simple Function\n",
    "def my_simple_function(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "# Example Working\n",
    "w1, w2 = 30, 60\n",
    "print(my_simple_function(w1, w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513c9dd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:09.860858Z",
     "iopub.status.busy": "2024-05-18T19:53:09.860050Z",
     "iopub.status.idle": "2024-05-18T19:53:09.868355Z",
     "shell.execute_reply": "2024-05-18T19:53:09.867133Z"
    },
    "papermill": {
     "duration": 0.085149,
     "end_time": "2024-05-18T19:53:09.870634",
     "exception": false,
     "start_time": "2024-05-18T19:53:09.785485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differentiating wrt to W1:  300.00000334\n",
      "Differentiating wrt to W2:  60.0000003\n"
     ]
    }
   ],
   "source": [
    "# Differentiating by tracking the difference in function by tweaking the variables\n",
    "eps = 1e-6\n",
    "w1, w2 = 30, 60\n",
    "\n",
    "diff_wrt_w1 = my_simple_function(w1 + eps, w2) - my_simple_function(w1, w2)\n",
    "diff_wrt_w2 = my_simple_function(w1, w2 + eps) - my_simple_function(w1, w2)\n",
    "print(\"Differentiating wrt to W1: \", round(diff_wrt_w1 / eps, 8))\n",
    "print(\"Differentiating wrt to W2: \", round(diff_wrt_w2 / eps, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713b0e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-17T08:35:31.844765Z",
     "iopub.status.busy": "2024-05-17T08:35:31.844328Z",
     "iopub.status.idle": "2024-05-17T08:35:31.852805Z",
     "shell.execute_reply": "2024-05-17T08:35:31.851564Z",
     "shell.execute_reply.started": "2024-05-17T08:35:31.844731Z"
    },
    "papermill": {
     "duration": 0.073084,
     "end_time": "2024-05-18T19:53:10.016431",
     "exception": false,
     "start_time": "2024-05-18T19:53:09.943347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Important**\n",
    "- Though the approximations are correct we need to call the `my_simple_function()` atleast one time for each parameter to attain the differentiation for each function.\n",
    "- This is not scalable for large and deep neural networks since there will be **tens of thousands of parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b3fae",
   "metadata": {
    "papermill": {
     "duration": 0.072243,
     "end_time": "2024-05-18T19:53:10.162115",
     "exception": false,
     "start_time": "2024-05-18T19:53:10.089872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Using the TensorFlow AutoDiff\n",
    "\n",
    "- The tf.Variables are tracked by the GradientTape once defined.\n",
    "- The gradient method from the TensorFlow AutoDiff goes through all the recorded computations once in reverse order.\n",
    "- Thus the TensorFlow AutoDiff is very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "468c7902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:10.308005Z",
     "iopub.status.busy": "2024-05-18T19:53:10.307603Z",
     "iopub.status.idle": "2024-05-18T19:53:10.322382Z",
     "shell.execute_reply": "2024-05-18T19:53:10.320930Z"
    },
    "papermill": {
     "duration": 0.090739,
     "end_time": "2024-05-18T19:53:10.324847",
     "exception": false,
     "start_time": "2024-05-18T19:53:10.234108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=300.0>, <tf.Tensor: shape=(), dtype=float32, numpy=60.0>]\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(30.), tf.Variable(60.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_simple_function(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9337451",
   "metadata": {
    "papermill": {
     "duration": 0.07211,
     "end_time": "2024-05-18T19:53:10.469621",
     "exception": false,
     "start_time": "2024-05-18T19:53:10.397511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Important**\n",
    "- Once the Tape is used it is automatically deleted to save memory, thus subsequent calls of the Tape variable leads to a runtime error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5da4d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:10.616228Z",
     "iopub.status.busy": "2024-05-18T19:53:10.615810Z",
     "iopub.status.idle": "2024-05-18T19:53:10.627354Z",
     "shell.execute_reply": "2024-05-18T19:53:10.626227Z"
    },
    "papermill": {
     "duration": 0.08773,
     "end_time": "2024-05-18T19:53:10.629762",
     "exception": false,
     "start_time": "2024-05-18T19:53:10.542032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=300.0>, <tf.Tensor: shape=(), dtype=float32, numpy=60.0>]\n",
      "The tape variable doesnt exist anymore\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = my_simple_function(w1, w2)\n",
    "\n",
    "gradient1 = tape.gradient(z, [w1, w2])\n",
    "print(gradient1)\n",
    "\n",
    "try:\n",
    "    gradient2 = tape.gradient(z, [w1, w2])\n",
    "except RuntimeError:\n",
    "    print(\"The tape variable doesnt exist anymore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af471d",
   "metadata": {
    "papermill": {
     "duration": 0.072519,
     "end_time": "2024-05-18T19:53:10.775428",
     "exception": false,
     "start_time": "2024-05-18T19:53:10.702909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Important**\n",
    "- We can make the tape persist untill we manually release the space allocated for the tape.\n",
    "- This way the tape can be called subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b73db47f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:10.923767Z",
     "iopub.status.busy": "2024-05-18T19:53:10.922670Z",
     "iopub.status.idle": "2024-05-18T19:53:10.937111Z",
     "shell.execute_reply": "2024-05-18T19:53:10.935765Z"
    },
    "papermill": {
     "duration": 0.091469,
     "end_time": "2024-05-18T19:53:10.939543",
     "exception": false,
     "start_time": "2024-05-18T19:53:10.848074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=300.0>, <tf.Tensor: shape=(), dtype=float32, numpy=60.0>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=300.0>, <tf.Tensor: shape=(), dtype=float32, numpy=60.0>]\n",
      "Thats two subsequent calls of the tape variable\n",
      "The tape has now been released\n",
      "Trying a third access of the tape ...\n",
      "The tape variable doesnt exist anymore\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = my_simple_function(w1, w2)\n",
    "\n",
    "gradient1 = tape.gradient(z, [w1, w2])\n",
    "print(gradient1)\n",
    "\n",
    "try:\n",
    "    gradient2 = tape.gradient(z, [w1, w2])\n",
    "    print(gradient2)\n",
    "except RuntimeError:\n",
    "    print(\"The tape variable doesnt exist anymore\")\n",
    "finally:\n",
    "    print(\"Thats two subsequent calls of the tape variable\")\n",
    "    del tape\n",
    "    print(\"The tape has now been released\")\n",
    "    \n",
    "try:\n",
    "    print(\"Trying a third access of the tape ...\")\n",
    "    gradient3 = tape.gradient(z, [w1, w2])\n",
    "    print(gradient3)\n",
    "except NameError:\n",
    "    print(\"The tape variable doesnt exist anymore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d552518",
   "metadata": {
    "papermill": {
     "duration": 0.075344,
     "end_time": "2024-05-18T19:53:11.087469",
     "exception": false,
     "start_time": "2024-05-18T19:53:11.012125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Important**\n",
    "- By default the tape only track operations of variables.\n",
    "- Thus operating on anything other than variables returns none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71c4a08e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:11.237190Z",
     "iopub.status.busy": "2024-05-18T19:53:11.236800Z",
     "iopub.status.idle": "2024-05-18T19:53:11.245403Z",
     "shell.execute_reply": "2024-05-18T19:53:11.244002Z"
    },
    "papermill": {
     "duration": 0.085986,
     "end_time": "2024-05-18T19:53:11.247623",
     "exception": false,
     "start_time": "2024-05-18T19:53:11.161637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "c1, c2, = tf.constant(30.), tf.constant(60.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = my_simple_function(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56fee7",
   "metadata": {
    "papermill": {
     "duration": 0.072683,
     "end_time": "2024-05-18T19:53:11.393482",
     "exception": false,
     "start_time": "2024-05-18T19:53:11.320799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Important**\n",
    "- The GradientTape can be forced to take gradient of other objects aswell by utilising the tf.watch()\n",
    "- The tf.watch() forces tape to keep a track of all the tensors that arent even variables.\n",
    "- They are then operated upon as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7090caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:11.543708Z",
     "iopub.status.busy": "2024-05-18T19:53:11.542834Z",
     "iopub.status.idle": "2024-05-18T19:53:11.551931Z",
     "shell.execute_reply": "2024-05-18T19:53:11.550688Z"
    },
    "papermill": {
     "duration": 0.086129,
     "end_time": "2024-05-18T19:53:11.554094",
     "exception": false,
     "start_time": "2024-05-18T19:53:11.467965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=300.0>, <tf.Tensor: shape=(), dtype=float32, numpy=60.0>]\n"
     ]
    }
   ],
   "source": [
    "c1, c2, = tf.constant(30.), tf.constant(60.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = my_simple_function(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398de23a",
   "metadata": {
    "papermill": {
     "duration": 0.072925,
     "end_time": "2024-05-18T19:53:11.700239",
     "exception": false,
     "start_time": "2024-05-18T19:53:11.627314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Important**\n",
    "- The gradients calculated by the GradientTape can also be stopped for different parts of the neural network during backprop where necessary.\n",
    "- This is done using the `tf.stop_gradient()` in the function that for which gradients are being taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bff1743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:11.849341Z",
     "iopub.status.busy": "2024-05-18T19:53:11.848581Z",
     "iopub.status.idle": "2024-05-18T19:53:11.861963Z",
     "shell.execute_reply": "2024-05-18T19:53:11.860219Z"
    },
    "papermill": {
     "duration": 0.090161,
     "end_time": "2024-05-18T19:53:11.864481",
     "exception": false,
     "start_time": "2024-05-18T19:53:11.774320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=180.0>, None]\n",
      "Here the second gradient returns none as it has been stopped during backprop\n"
     ]
    }
   ],
   "source": [
    "def my_stopping_fn(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "w1, w2 = tf.Variable(30.), tf.Variable(60.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_stopping_fn(w1, w2)\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(gradients)\n",
    "print(\"Here the second gradient returns none as it has been stopped during backprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958b850",
   "metadata": {
    "papermill": {
     "duration": 0.073415,
     "end_time": "2024-05-18T19:53:12.012546",
     "exception": false,
     "start_time": "2024-05-18T19:53:11.939131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Important**\n",
    "- Given a list of losses the gradients are calculated for all the elements in the list together.\n",
    "- To avoid this we utilise the `jacobian()` which calculates the derivative for each element of the list individually.\n",
    "- Partial derivatives can take a step further and also calculates `hessians()`.\n",
    "- Both the `jacobian()` and the `hessian()` are provided in the **`GradientTape of TensorFlow`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cf4f265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:12.164054Z",
     "iopub.status.busy": "2024-05-18T19:53:12.163631Z",
     "iopub.status.idle": "2024-05-18T19:53:12.187731Z",
     "shell.execute_reply": "2024-05-18T19:53:12.186257Z"
    },
    "papermill": {
     "duration": 0.102771,
     "end_time": "2024-05-18T19:53:12.190134",
     "exception": false,
     "start_time": "2024-05-18T19:53:12.087363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of the List (Automated Sum)\n",
      " [<tf.Tensor: shape=(), dtype=float32, numpy=901.8>, <tf.Tensor: shape=(), dtype=float32, numpy=180.0>]\n",
      "\n",
      "Gradients of the list (Summed together)\n",
      " [<tf.Tensor: shape=(), dtype=float32, numpy=901.8>, <tf.Tensor: shape=(), dtype=float32, numpy=180.0>]\n"
     ]
    }
   ],
   "source": [
    "# Taking the sum of elements of list for calculating the gradient\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = my_simple_function(w1, w2 + .1)\n",
    "    z2 = my_simple_function(w1, w2 + .3)\n",
    "    z3 = my_simple_function(w1, w2 + .5)\n",
    "    \n",
    "gradients_without_summation = tape.gradient([z1, z2, z3], [w1, w2])\n",
    "print(\"Gradients of the List (Automated Sum)\\n\", gradients_without_summation)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = my_simple_function(w1, w2 + .1)\n",
    "    z2 = my_simple_function(w1, w2 + .3)\n",
    "    z3 = my_simple_function(w1, w2 + .5)\n",
    "    z = z1 + z2 + z3\n",
    "    \n",
    "gradients_with_summation = tape.gradient(z, [w1, w2])\n",
    "print(\"\\nGradients of the list (Summed together)\\n\", gradients_with_summation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f655fdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:12.339206Z",
     "iopub.status.busy": "2024-05-18T19:53:12.338230Z",
     "iopub.status.idle": "2024-05-18T19:53:12.351222Z",
     "shell.execute_reply": "2024-05-18T19:53:12.349898Z"
    },
    "papermill": {
     "duration": 0.08994,
     "end_time": "2024-05-18T19:53:12.353377",
     "exception": false,
     "start_time": "2024-05-18T19:53:12.263437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated Jacobians\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=300.0>, <tf.Tensor: shape=(), dtype=float32, numpy=60.0>]\n",
      "\n",
      "The calculated Hessians\n",
      "[[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=2.0>], [<tf.Tensor: shape=(), dtype=float32, numpy=2.0>, None]]\n"
     ]
    }
   ],
   "source": [
    "# Computing Jacobians and Hessians\n",
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = my_simple_function(w1, w2)\n",
    "    \n",
    "    # Calculating the Jacobians\n",
    "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
    "    print(\"The calculated Jacobians\")\n",
    "    print(jacobians)\n",
    "    \n",
    "# Calculating the Jacobians\n",
    "hessians = [hessian_tape.gradient(jacobian, [w1, w2]) for jacobian in jacobians]\n",
    "print(\"\\nThe calculated Hessians\")\n",
    "print(hessians)\n",
    "\n",
    "# Deleting the Gradient Tape\n",
    "del hessian_tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ceb47",
   "metadata": {
    "papermill": {
     "duration": 0.073223,
     "end_time": "2024-05-18T19:53:12.499975",
     "exception": false,
     "start_time": "2024-05-18T19:53:12.426752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Working with Custom Training Loops**\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfa1dd4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:12.648077Z",
     "iopub.status.busy": "2024-05-18T19:53:12.647339Z",
     "iopub.status.idle": "2024-05-18T19:53:12.943957Z",
     "shell.execute_reply": "2024-05-18T19:53:12.942595Z"
    },
    "papermill": {
     "duration": 0.37394,
     "end_time": "2024-05-18T19:53:12.946756",
     "exception": false,
     "start_time": "2024-05-18T19:53:12.572816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Simple Model\n",
    "my_simple_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Random Sampling of the Training set to batch examples together\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    \n",
    "    # Acquiring indexs of the required batch size\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    \n",
    "    # Returning the subsets in the dataset\n",
    "    return X[idx], y[idx]\n",
    "    \n",
    "# A function to display the status of training taking place\n",
    "def status_bar(step, total, loss, metrics=None):\n",
    "    \n",
    "    # Prints the metrics as long as they are not none along with the losses for each epoch\n",
    "    metrics = \" - \".join([f\"{metric.name}: {metric.result():.4f}\" for metric in [loss] + (metrics or [])])\n",
    "    \n",
    "    # New line if training has completed else space\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    \n",
    "    # Result string\n",
    "    print(f\"\\r{step}/{total} - {metrics}\", end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eeae1bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:53:13.099270Z",
     "iopub.status.busy": "2024-05-18T19:53:13.098468Z",
     "iopub.status.idle": "2024-05-18T19:56:37.230742Z",
     "shell.execute_reply": "2024-05-18T19:56:37.229763Z"
    },
    "papermill": {
     "duration": 204.211489,
     "end_time": "2024-05-18T19:56:37.233253",
     "exception": false,
     "start_time": "2024-05-18T19:53:13.021764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20\n",
      "362/362 - mean: 1.6523 - mean_absolute_error: 0.9271\n",
      "2/20\n",
      "362/362 - mean: 1.1951 - mean_absolute_error: 0.7728\n",
      "3/20\n",
      "362/362 - mean: 0.9898 - mean_absolute_error: 0.6975\n",
      "4/20\n",
      "362/362 - mean: 0.8714 - mean_absolute_error: 0.6506\n",
      "5/20\n",
      "362/362 - mean: 0.7924 - mean_absolute_error: 0.6192\n",
      "6/20\n",
      "362/362 - mean: 0.7348 - mean_absolute_error: 0.5957\n",
      "7/20\n",
      "362/362 - mean: 0.6892 - mean_absolute_error: 0.5771\n",
      "8/20\n",
      "362/362 - mean: 0.6564 - mean_absolute_error: 0.5628\n",
      "9/20\n",
      "362/362 - mean: 0.6287 - mean_absolute_error: 0.5506\n",
      "10/20\n",
      "362/362 - mean: 0.6084 - mean_absolute_error: 0.5413\n",
      "11/20\n",
      "362/362 - mean: 0.5933 - mean_absolute_error: 0.5328\n",
      "12/20\n",
      "362/362 - mean: 0.5772 - mean_absolute_error: 0.5257\n",
      "13/20\n",
      "362/362 - mean: 0.5631 - mean_absolute_error: 0.5194\n",
      "14/20\n",
      "362/362 - mean: 0.5503 - mean_absolute_error: 0.5136\n",
      "15/20\n",
      "362/362 - mean: 0.5385 - mean_absolute_error: 0.5083\n",
      "16/20\n",
      "362/362 - mean: 0.5281 - mean_absolute_error: 0.5034\n",
      "17/20\n",
      "362/362 - mean: 0.5196 - mean_absolute_error: 0.4994\n",
      "18/20\n",
      "362/362 - mean: 0.5108 - mean_absolute_error: 0.4953\n",
      "19/20\n",
      "362/362 - mean: 0.5029 - mean_absolute_error: 0.4917\n",
      "20/20\n",
      "362/362 - mean: 0.4961 - mean_absolute_error: 0.4885\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the model\n",
    "n_epochs = 20\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# Training Loop\n",
    "# Looping for n_epochs\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"{epoch}/{n_epochs}\")\n",
    "    \n",
    "    # Looping for n_steps per epoch\n",
    "    for step in range(1, n_steps + 1):\n",
    "        \n",
    "        # Sampling the batches for each epoch\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Making predictions for the current batch\n",
    "            y_pred = my_simple_model(X_batch, training=True)\n",
    "            \n",
    "            # Calculating the Squared Loss for the current batch\n",
    "            squared_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            \n",
    "            # Total loss alongside the regularisation losses\n",
    "            loss = tf.add_n([squared_loss] + my_simple_model.losses)\n",
    "            \n",
    "        # Calculating the Gradients\n",
    "        gradients = tape.gradient(loss, my_simple_model.trainable_variables)\n",
    "        \n",
    "        # Updating the weights of in backprop\n",
    "        optimizer.apply_gradients(zip(gradients, my_simple_model.trainable_variables))\n",
    "        \n",
    "        # Accounting for Constraints\n",
    "        for variable in my_simple_model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "                \n",
    "        # Displaying the status\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        status_bar(step, n_steps, mean_loss, metrics)\n",
    "        \n",
    "    # Resetting the states of the streamline metrics for the next epoch\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4b931",
   "metadata": {
    "papermill": {
     "duration": 0.172036,
     "end_time": "2024-05-18T19:56:37.577433",
     "exception": false,
     "start_time": "2024-05-18T19:56:37.405397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Working with TensorFlow Functions and Graphs**\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83a1f15b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:56:37.925424Z",
     "iopub.status.busy": "2024-05-18T19:56:37.924758Z",
     "iopub.status.idle": "2024-05-18T19:56:38.017775Z",
     "shell.execute_reply": "2024-05-18T19:56:38.016543Z"
    },
    "papermill": {
     "duration": 0.270415,
     "end_time": "2024-05-18T19:56:38.019928",
     "exception": false,
     "start_time": "2024-05-18T19:56:37.749513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace of the python function:\n",
      " <function simple_function at 0x7b745c388a60>\n",
      "\n",
      "Result for integers:\n",
      " 341\n",
      "\n",
      "Result for TensorFlow variables:\n",
      " tf.Tensor(341, shape=(), dtype=int32)\n",
      "\n",
      "Namespace of the tensorflow function:\n",
      " <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function object at 0x7b745c4fc160>\n",
      "\n",
      "Result for integers:\n",
      " tf.Tensor(341, shape=(), dtype=int32)\n",
      "\n",
      "Result for TensorFlow variables:\n",
      " tf.Tensor(341, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Simple Python function\n",
    "def simple_function(x, y):\n",
    "    return 3 * x ** 2 + 2 * y + 1\n",
    "\n",
    "print(\"Namespace of the python function:\\n\", simple_function)\n",
    "print(\"\\nResult for integers:\\n\", simple_function(10, 20))\n",
    "print(\"\\nResult for TensorFlow variables:\\n\", simple_function(tf.Variable(10), tf.Variable(20)))\n",
    "\n",
    "# Converting the Python Function to a TensorFlow Function\n",
    "tf_simple_function = tf.function(simple_function)\n",
    "\n",
    "print(\"\\nNamespace of the tensorflow function:\\n\", tf_simple_function)\n",
    "print(\"\\nResult for integers:\\n\", tf_simple_function(10, 20))\n",
    "print(\"\\nResult for TensorFlow variables:\\n\", tf_simple_function(tf.Variable(10), tf.Variable(20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956fac6",
   "metadata": {
    "papermill": {
     "duration": 0.170459,
     "end_time": "2024-05-18T19:56:38.362791",
     "exception": false,
     "start_time": "2024-05-18T19:56:38.192332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Importance of TensorFlow Functions**\n",
    "--\n",
    "\n",
    "- TensorFlow functions work by creating computation graphs for each of the operations taking place within the function.\n",
    "- They automatically simplify the operations and prune unused nodes to increase efficiency.\n",
    "- Thus are often much faster than regular python functions as they support parallel execution by default.\n",
    "- Therefore complex expressions, functions are execute much better when executed as TensorFlow functions.\n",
    "\n",
    "\n",
    "**Properties of TensorFlow Functions**\n",
    "--\n",
    "\n",
    "- When passing tensors to TensorFlow function, they further try to reuse already created computation graphs where possible through polymorphism after breaking down the expressions.\n",
    "- It any python datastructure is passed into a TensorFlow function a new computation graph is created for each call of the TensorFlow function.\n",
    "- This leads to loss RAM and storage quickly, hence any TensorFlow function once used must be deallocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e474b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:56:38.705601Z",
     "iopub.status.busy": "2024-05-18T19:56:38.705224Z",
     "iopub.status.idle": "2024-05-18T19:56:38.790193Z",
     "shell.execute_reply": "2024-05-18T19:56:38.788811Z"
    },
    "papermill": {
     "duration": 0.259107,
     "end_time": "2024-05-18T19:56:38.792730",
     "exception": false,
     "start_time": "2024-05-18T19:56:38.533623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name space of the function:\n",
      " <tensorflow.python.eager.polymorphic_function.polymorphic_function.Function object at 0x7b7454328be0>\n",
      "\n",
      "Result for Integers:\n",
      " tf.Tensor(-12, shape=(), dtype=int32)\n",
      "\n",
      "Result for TensorFlow Variables:\n",
      " tf.Tensor(-12, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Python functions can also be decorated into TensorFlow functions\n",
    "@tf.function\n",
    "def new_simple_function(x, y, z):\n",
    "    return x ** 2 + y ** 2 - z ** 2\n",
    "\n",
    "print(\"Name space of the function:\\n\", new_simple_function)\n",
    "print(\"\\nResult for Integers:\\n\", new_simple_function(2, 3, 5))\n",
    "print(\"\\nResult for TensorFlow Variables:\\n\", new_simple_function(tf.Variable(2), tf.Variable(3), tf.Variable(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1caf0fd",
   "metadata": {
    "papermill": {
     "duration": 0.173652,
     "end_time": "2024-05-18T19:56:39.155891",
     "exception": false,
     "start_time": "2024-05-18T19:56:38.982239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Important**\n",
    "- The python function can still be accessed from the TensorFlow function using the python_function attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "667eac6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T19:56:39.505313Z",
     "iopub.status.busy": "2024-05-18T19:56:39.504897Z",
     "iopub.status.idle": "2024-05-18T19:56:39.511673Z",
     "shell.execute_reply": "2024-05-18T19:56:39.510549Z"
    },
    "papermill": {
     "duration": 0.185176,
     "end_time": "2024-05-18T19:56:39.514065",
     "exception": false,
     "start_time": "2024-05-18T19:56:39.328889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_simple_function.python_function(2, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdb4cf",
   "metadata": {
    "papermill": {
     "duration": 0.172065,
     "end_time": "2024-05-18T19:56:39.857185",
     "exception": false,
     "start_time": "2024-05-18T19:56:39.685120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Fin ✨**\n",
    "--\n",
    "\n",
    "## I had a lot of fun working on this notebook\n",
    "## Hope you had just as much fun reading\n",
    "🙂🙃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61dceba",
   "metadata": {
    "papermill": {
     "duration": 0.238544,
     "end_time": "2024-05-18T19:56:40.272447",
     "exception": false,
     "start_time": "2024-05-18T19:56:40.033903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 283.348071,
   "end_time": "2024-05-18T19:56:42.909072",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-18T19:51:59.561001",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
