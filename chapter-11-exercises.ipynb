{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf56268a",
   "metadata": {
    "papermill": {
     "duration": 0.005227,
     "end_time": "2024-07-05T20:33:22.927390",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.922163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Question - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5becd899",
   "metadata": {
    "papermill": {
     "duration": 0.004121,
     "end_time": "2024-07-05T20:33:22.936225",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.932104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization ?\n",
    "\n",
    "> No, even if using He Initialisation for creating kernels with the same value, the kernel will fail to break the symmetry problem.\n",
    "> This causes the deep neural network to learn patterns much slower and is prone to exploding or vanishing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29aa865",
   "metadata": {
    "papermill": {
     "duration": 0.004066,
     "end_time": "2024-07-05T20:33:22.944667",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.940601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Question - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b759d3a",
   "metadata": {
    "papermill": {
     "duration": 0.004129,
     "end_time": "2024-07-05T20:33:22.953081",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.948952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "> Yes, it is okay to initialise all the bias terms to 0 since the contribution made through bias vector initialisation is almost indifferent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685d264",
   "metadata": {
    "papermill": {
     "duration": 0.004028,
     "end_time": "2024-07-05T20:33:22.961515",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.957487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Question - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00a582",
   "metadata": {
    "papermill": {
     "duration": 0.003994,
     "end_time": "2024-07-05T20:33:22.969824",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.965830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Name three advantages of the SELU activation function over ReLU ?\n",
    "\n",
    "> - The Selu activation function is much faster than Relu for standardised input information.\n",
    "> - The Selu activation function ensures that the exploding and vanishing gradients problem is always at bay while training the neural network.\n",
    "> - The Selu activation function makes the Neural Network self normalising ensuring all the parameters learnt by the model during training are robust for all features and instances of the training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26148bdb",
   "metadata": {
    "papermill": {
     "duration": 0.004169,
     "end_time": "2024-07-05T20:33:22.978278",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.974109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Question - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e858601",
   "metadata": {
    "papermill": {
     "duration": 0.004065,
     "end_time": "2024-07-05T20:33:22.986747",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.982682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax ?\n",
    "\n",
    "> - Selu: When the input data is standardised and the neural network contains many hidden layers\n",
    "> - Leaky Relu (and its variants): When the dataset is large but slight addition in computation time can be traded for a significant improvement in performance.\n",
    "> - Relu: When the usage of a mostly non saturating activation function doesnt hurt the performance of the model.\n",
    "> - Tanh: When the output layer of a model is within the range of -1 to 1.\n",
    "> - Logistic: When the output layer of the model is binary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb599f",
   "metadata": {
    "papermill": {
     "duration": 0.004027,
     "end_time": "2024-07-05T20:33:22.995058",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.991031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Question - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa070ee",
   "metadata": {
    "papermill": {
     "duration": 0.004126,
     "end_time": "2024-07-05T20:33:23.003484",
     "exception": false,
     "start_time": "2024-07-05T20:33:22.999358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "> - When the momentum hyperparameter is set much closer to 1 such as 0.9999999 ... the Gradient descent algorithm tends to pickup speed.\n",
    "> - However since the Gradient Descent algorithm isn't the most efficient it more often then not tends to overshoot the global optimum.\n",
    "> - It continues to oscillate around the global optimum until either reaching it precisely or ending up overshooting.\n",
    "> - Thus all together the algorithm takes much longer to converge with a larger value of momentum than a small value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d8a67",
   "metadata": {
    "papermill": {
     "duration": 0.004467,
     "end_time": "2024-07-05T20:33:23.012340",
     "exception": false,
     "start_time": "2024-07-05T20:33:23.007873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Question - 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b203a",
   "metadata": {
    "papermill": {
     "duration": 0.004113,
     "end_time": "2024-07-05T20:33:23.020938",
     "exception": false,
     "start_time": "2024-07-05T20:33:23.016825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Name three ways you can produce a sparse model.\n",
    "\n",
    "> - Train a model as usual and then push all the tiny weights to 0.\n",
    "> - Apply l1 regularisation to zero out parameters which are not being used in learning patterns during training itself.\n",
    "> - Utilise the TensorFlow model development toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b998f9",
   "metadata": {
    "papermill": {
     "duration": 0.004163,
     "end_time": "2024-07-05T20:33:23.029526",
     "exception": false,
     "start_time": "2024-07-05T20:33:23.025363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Question - 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea5ea8",
   "metadata": {
    "papermill": {
     "duration": 0.004029,
     "end_time": "2024-07-05T20:33:23.037913",
     "exception": false,
     "start_time": "2024-07-05T20:33:23.033884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "> - Yes, dropout does slowdown training as the model takes longer to regularise all its parameters and approach convergence.\n",
    "> - When using Monte Carlo Simulations the model utilises dropout layers while making inferences. Thus the model is much slower not just while training but while making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce4ff1",
   "metadata": {
    "papermill": {
     "duration": 0.004048,
     "end_time": "2024-07-05T20:33:23.046413",
     "exception": false,
     "start_time": "2024-07-05T20:33:23.042365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.441836,
   "end_time": "2024-07-05T20:33:25.678281",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-05T20:33:19.236445",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
